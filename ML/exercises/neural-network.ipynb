{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2ff10206","cell_type":"code","source":"from copy import deepcopy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom utils import load_house_data, plot_housing_prices, animate_neural_network\n\nsizes, prices, labels = load_house_data('data/housing_prices.txt')\n\n# As before, we load the housing data and scale it\nsize_scaled = (sizes - np.mean(sizes)) / np.std(sizes)\nprice_scaled = (prices - np.mean(prices)) / np.std(prices)\n\n# Create feature list: [size, price, size^2, price^2]\nfeatures = [\n    size_scaled,      # Feature 0: size\n    price_scaled,     # Feature 1: price\n]\n\n# Convert to numpy array for easier computation\nfeature_matrix = np.array(features).T  # Shape: (n_samples, n_features)\nfeature_names = ['size', 'price']","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"id":"6f317752","cell_type":"code","source":"def sigmoid(z):\n    \"\"\"Sigmoid activation function\"\"\"\n    z = np.clip(z, -500, 500)\n    return 1 / (1 + np.exp(-z))\n\n\ndef sigmoid_derivative(z):\n    \"\"\"Derivative of sigmoid function\"\"\"\n    ####### YOUR CODE HERE #######\n    s = ...\n    ds = ...\n    ###### END OF YOUR CODE ######\n    return ds\n\n\nclass SimpleNeuralNetwork:\n    \"\"\"A simple 2-layer neural network for binary classification\"\"\"\n    \n    def __init__(self, input_size=2, hidden_size=6, output_size=1, activation='sigmoid'):\n        \"\"\"\n        Initialize the neural network\n        \n        Parameters:\n        - input_size: number of input features\n        - hidden_size: number of neurons in hidden layer\n        - output_size: number of output neurons (1 for binary classification)\n        - activation: 'sigmoid' or 'relu' for hidden layer activation\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.activation = activation\n        \n        # Initialize weights randomly and and biases as zeros\n\n        ####### YOUR CODE HERE #######\n        # Hidden layer weights: (input_size, hidden_size)\n        self.W1 = ...\n        self.b1 = ...\n        \n        # Output layer weights: (hidden_size, output_size)\n        self.W2 = ...\n        self.b2 = ...\n        ###### END OF YOUR CODE ######\n        \n        # Choose activation function\n        self.activation_func = sigmoid\n        self.activation_derivative = sigmoid_derivative\n        \n        # Store training history\n        self.loss_history = []\n        self.weights_history = {'W1': [], 'b1': [], 'W2': [], 'b2': []}\n\n    \n    def forward_pass(self, X):\n        \"\"\"\n        Perform forward propagation through the network\n        \n        Parameters:\n        - X: input features (n_samples, n_features)\n        \n        Returns:\n        - A2: output predictions\n        - cache: intermediate values for backpropagation\n        \"\"\"\n        ##### YOUR CODE HERE #####\n        # Layer 1: Hidden layer\n        # Linear transformation: Z1 = X @ W1 + b1\n        Z1 = ...  # Shape: (n_samples, hidden_size)\n        \n        # Apply activation function\n        A1 = ...      # Shape: (n_samples, hidden_size)\n        \n        # Layer 2: Output layer\n        # Linear transformation: Z2 = A1 @ W2 + b2\n        Z2 = ... # Shape: (n_samples, output_size)\n        \n        # Apply sigmoid for binary classification\n        A2 = ...                   # Shape: (n_samples, output_size)\n        ######### END OF YOUR CODE ######\n        \n        # Store intermediate values for backpropagation\n        cache = {\n            'X': X,\n            'Z1': Z1,\n            'A1': A1,\n            'Z2': Z2,\n            'A2': A2\n        }\n        \n        return A2, cache\n    \n    def calculate_loss(self, y_true, y_pred):\n        \"\"\"Calculate binary cross-entropy loss\"\"\"\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        return loss\n    \n    def backward_pass(self, cache, y_true):\n        \"\"\"\n        Perform backpropagation to calculate gradients\n        \n        Parameters:\n        - cache: intermediate values from forward pass\n        - y_true: true labels\n        \n        Returns:\n        - gradients: dictionary containing all gradients\n        \"\"\"\n        # Retrieve cached values\n        ####### YOUR CODE HERE #######\n        X, Z1, A1, Z2, A2 = ...\n        m = X.shape[0]  # number of samples\n        \n        # Output layer gradients\n        # dL/dZ2 = A2 - y_true (derivative of sigmoid + cross-entropy)\n        # reshape y_true to match A2 shape via y_true.reshape(-1, 1)\n        dZ2 = ...  # Shape: (n_samples, 1)\n        \n        # dL/dW2 = A1.T @ dZ2 / m\n        dW2 = ...        # Shape: (hidden_size, 1)\n        \n        # dL/db2 = mean(dZ2) (over the first axis and keep dimensions)\n        db2 = ...  # Shape: (1, 1)\n        \n        # Hidden layer gradients\n        # dL/dA1 = dZ2 @ W2.T\n        dA1 = ...      # Shape: (n_samples, hidden_size)\n        \n        # dL/dZ1 = dA1 * activation_derivative(Z1)\n        dZ1 = ...  # Shape: (n_samples, hidden_size)\n        \n        # dL/dW1 = X.T @ dZ1 / m\n        dW1 = ...        # Shape: (input_size, hidden_size)\n        \n        # dL/db1 = mean(dZ1) (over the first axis and keep dimensions)\n        db1 = ...  # Shape: (1, hidden_size)\n        ######### END OF YOUR CODE ######\n        \n        gradients = {\n            'dW1': dW1,\n            'db1': db1,\n            'dW2': dW2,\n            'db2': db2\n        }\n        \n        return gradients\n    \n\n    def update_parameters(self, gradients, learning_rate):\n        \"\"\"Update network parameters using gradients\"\"\"\n        self.W1 -= learning_rate * gradients['dW1']\n        self.b1 -= learning_rate * gradients['db1']\n        self.W2 -= learning_rate * gradients['dW2']\n        self.b2 -= learning_rate * gradients['db2']\n\n    def predict(self, X):\n        \"\"\"Get prediction probabilities\"\"\"\n        y_pred, _ = self.forward_pass(X)\n        return y_pred.flatten()    \n\n    def train(self, X, y, learning_rate=0.1, n_iterations=1000):\n        \"\"\"\n        Train the neural network using gradient descent\n        Parameters:\n        - X: input features\n        - y: true labels\n        - learning_rate: step size for parameter updates\n        - n_iterations: number of training iterations\n        Returns:\n        - loss history\n        - weights history\n        \"\"\"\n        loss_history = []\n        weights_history = []\n        for i in range(n_iterations):\n            # Forward propagation\n            y_pred, cache = self.forward_pass(X)\n            \n            # Calculate loss\n            loss = self.calculate_loss(y, y_pred.flatten())\n            \n            # Backward propagation\n            gradients = self.backward_pass(cache, y)\n            \n            # Update parameters\n            self.update_parameters(gradients, learning_rate)\n            \n            # Store training history\n            loss_history.append(loss)\n            weights_history.append({\n                'W1': self.W1.copy(),\n                'b1': self.b1.copy(),\n                'W2': self.W2.copy(),\n                'b2': self.b2.copy()\n            })\n            \n            # Print progress\n            if i % 100 == 0:\n                print(f\"Iteration {i}, Loss: {loss:.6f}\")\n                print(f\"Sample gradients - dW1[0,0]: {gradients['dW1'][0,0]:.6f}, \"\n                      f\"dW2[0,0]: {gradients['dW2'][0,0]:.6f}\")\n\n        return loss_history, weights_history    \n","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"id":"848f8f70","cell_type":"code","source":"# You can use this cell to test if the foward pass works correctly\nnn = SimpleNeuralNetwork(input_size=2, hidden_size=6, output_size=1, activation='sigmoid')\ny_pred, cache = nn.forward_pass(feature_matrix)\nprint(\"Predictions shape:\", y_pred.shape)\nprint(\"First 5 predictions:\", y_pred[:5].flatten())","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"id":"de70ee95","cell_type":"code","source":"# Train the neural network\nnn = SimpleNeuralNetwork(\n    input_size=feature_matrix.shape[1], \n    hidden_size=6, \n    output_size=1,\n    activation='sigmoid'\n)\n\nprint(\"Training Neural Network...\")\nloss_history, param_history = nn.train(feature_matrix, labels, learning_rate=1.5, n_iterations=500)\n\n# Make predictions\npredictions = nn.predict(feature_matrix)\n\n# Calculate accuracy\naccuracy = np.mean((predictions > 0.5) == labels)\nprint(f\"\\nFinal Training Accuracy: {accuracy:.4f}\")\n\nlambdas = []\nfor params in param_history:\n    temp_nn = SimpleNeuralNetwork(\n        input_size=feature_matrix.shape[1],\n        hidden_size=6,\n        output_size=1,\n        activation='sigmoid'\n    )\n    temp_nn.W1 = params['W1']\n    temp_nn.b1 = params['b1']\n    temp_nn.W2 = params['W2']\n    temp_nn.b2 = params['b2']\n\n    lambdas.append(deepcopy(temp_nn.predict))\n\n# Create and display animation\nprint(\"\\nCreating training animation...\")\nanim = animate_neural_network(\n    feature_matrix, lambdas, labels, loss_history,\n    feature_names, plot_every=10,\n    # save_path=\"output/neural_network_training.mp4\"\n)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())\n","metadata":{"trusted":false},"outputs":[],"execution_count":null}]}